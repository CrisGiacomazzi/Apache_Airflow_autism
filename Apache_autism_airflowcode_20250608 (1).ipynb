{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHfYrPsukrvv"
      },
      "outputs": [],
      "source": [
        "# pip install airflow\n",
        "\n",
        "!pip install apache-airflow -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from airflow import DAG\n",
        "from airflow.providers.standard.operators.bash import BashOperator\n",
        "from airflow.providers.standard.operators.python import PythonOperator\n",
        "from datetime import datetime, date\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import os"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_C7012KlQpq",
        "outputId": "45291e40-5f49-4368-8af6-70b86e1fd288"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-06-08T22:15:17.009+0000] {utils.py:162} INFO - NumExpr defaulting to 2 threads.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining DAG\n",
        "# tag = categorize and organize DAG\n",
        "\n",
        "def scrape_autism_alliance_news():\n",
        "    \"\"\"\n",
        "    Scrapes news from autismalliance.ca, filters for 'autism'/'austim' in title\n",
        "    and future dates, and saves to a CSV.\n",
        "    \"\"\"\n",
        "    url = 'https://autismalliance.ca/news/?news-topic=&year=&page=1'\n",
        "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}\n",
        "\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers, timeout=10) # Added timeout\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors (4xx or 5xx)\n",
        "\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        title_elements = soup.find_all('a', class_='archive-article__link')\n",
        "        date_elements = soup.find_all('p', class_='archive-article__date')\n",
        "\n",
        "        results = []\n",
        "        today = date.today() # Get today's date once\n",
        "\n",
        "        for title_tag, date_tag in zip(title_elements, date_elements):\n",
        "            title_text = title_tag.get_text(strip=True)\n",
        "            date_text = date_tag.get_text(strip=True)\n",
        "\n",
        "            # Convert string date to datetime object\n",
        "            try:\n",
        "                pub_date = datetime.strptime(date_text, '%B %d, %Y').date()\n",
        "            except ValueError:\n",
        "                print(f\"Skipping article due to invalid date format: {date_text}\")\n",
        "                continue\n",
        "\n",
        "            # Apply filters\n",
        "            if ('autism' in title_text.lower() or 'austim' in title_text.lower()) and \\\n",
        "               (pub_date > today): # Filter for future dates\n",
        "                results.append({\n",
        "                    'title': title_text,\n",
        "                    'date': pub_date,\n",
        "                    'url': title_tag['href']\n",
        "                })\n",
        "\n",
        "        output_dir = '/opt/airflow/dags/scraped_data'\n",
        "        csv_filename = 'autism_alliance_news.csv'\n",
        "        full_csv_path = os.path.join(output_dir, csv_filename)\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        if results:\n",
        "            df = pd.DataFrame(results)\n",
        "            file_exists = os.path.exists(full_csv_path)\n",
        "            df.to_csv(full_csv_path, mode='a', header=not file_exists, index=False)\n",
        "            print(f\"{len(results)} new article(s) collected and saved to {full_csv_path}.\")\n",
        "        else:\n",
        "            print(\"No new articles with 'autism/austim' and a future date found.\")\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during web request: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred during scraping: {e}\")\n",
        "\n",
        "# --- Airflow DAG Definition ---\n",
        "\n",
        "with DAG(\n",
        "    dag_id='autism_alliance_news_scraper',\n",
        "    start_date=datetime(2025, 6, 9),\n",
        "    schedule='@weekly',\n",
        "    catchup=False,\n",
        "    tags=['web_scraping', 'news', 'autism_alliance'],\n",
        ") as dag:\n",
        "    start_task = BashOperator(\n",
        "        task_id='start_scraping_process',\n",
        "        bash_command='echo \"Starting web scraping process...\"',\n",
        "    )\n",
        "\n",
        "    scrape_news_task = PythonOperator(\n",
        "        task_id='scrape_autism_alliance_news_data',\n",
        "        python_callable=scrape_autism_alliance_news,\n",
        "    )\n",
        "\n",
        "    end_task = BashOperator(\n",
        "        task_id='finish_scraping_process',\n",
        "        bash_command='echo \"Web scraping process finished.\"',\n",
        "    )\n",
        "\n",
        "    start_task >> scrape_news_task >> end_task"
      ],
      "metadata": {
        "id": "ECt6C1cqlXFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ifQ0MyrvlkrH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}